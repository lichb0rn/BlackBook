---
aliases: ["Big-O", "О-большое", "о-малое"]
---
MOC: [[040 Computer Science]], [[Algorithms]], [[023 Математический Анализ]]

---

# Big O 

- Constant time : *$O(1)$*. This is, obviously, the best you can achieve asymptotically, but since it does not depend on the input size, we very rarely see algorithms running in this time
- Logarithmic time : *$O(\log n)$*. We saw that binary search was one such algorithm. Generally, we see this complexity when we can reduce the size of the input we look at by a fixed fraction in each iteration of a loop. Typically, we can cut the data in half, and we get a base-two logarithm, but since the difference between two different-based logarithms is a constant, *$\log a(x) = 1 / \log b(a)\log b(x)$*, we rarely write the base in big-Oh notation.
- Linear time : *$O(n)$*. We saw several examples of linear-time algorithms in this chapter. Whenever we do something where we have to, in the worst case, examine all our input, the complexity will be at least this.
- Log-linear time : *$O(n\log n)$*
- Quadratic time : $O(n^2)$*. We saw a sorting algorithm with this complexity. This complexity often shows up when we have nested loops.
- Cubic time : *$O(n^3)$*. This is another class we haven’t seen yet, but when we have three levels of nested loops, we see it. If you multiply two n × n algorithms the straightforward way, $C = AB$, you have to compute $n × n$ values in $C$, and for each compute *$c_{ij}=\sum_{k}a_{ik}b_{kj}$* where you add n values, giving you a total running time of $n^3$.
- Exponential time : *$O(2^n)$*. You should avoid this complexity like the plague. Even for tiny n, this running time is practically forever. It does, unfortunately, pop up in many important optimization problems where we do not know of any faster algorithms. If you have a problem that can only be solved in this complexity, you should try to modify the problem to something you can solve more efficiently, or you should try to approximate the solution instead of getting the optimal solution.

![Bigo graph](https://he-s3.s3.amazonaws.com/media/uploads/ece920b.png)


# Асимптотическое поведение функции
**«O» большое** и **«o» малое** — математические обозначения для сравнения асимптотического поведения (асимптотики) [[Функция|функций]].

$o(f)$ обозначает бесконечно малое относительно $f$. $O(f)$ растёт не быстрее, чем $f$.

 Пусть *$f(x)$* и *$g(x)$* - две функции, определенные в [[Окрестность|проколотой окрестности]] точки $x_0$. Тогда говорят, что:
- $f$ является *$O$ большим* от $g$ при $x \to x_0$, если существует такая константа $C>0$, что для всех $x$ из некоторой окрестности точки $x_0$ имеет место неравенство *$$|f(x)| \leq C|g(x)|$$* 
- $f$ является *$o$ малым* от $g$ при $x \to x_0$, если для любого $\epsilon > 0$ найдётся такая проколотая окрестность $U_{x_0}'$ точки $x_0$, что для всех $x \in U_{x_0}'$ имеет место неравенство *$$|f(x)| < \epsilon|g(x)|$$
## Сложение функций

Сложение двух функций определяется доминантной функцией:
$$O(f(n)) + O(g(n)) = O(max(f(n), g)n))$$
$$\Omega(f(n)) + \Omega(g(n)) = \Omega(max(f(n), g)n))$$
$$\Theta(f(n)) + \Theta(g(n)) = \Theta(max(f(n), g)n))$$
